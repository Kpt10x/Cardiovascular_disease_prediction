# -*- coding: utf-8 -*-
"""Cardiovascular .ipynb

Automatically generated by Colab.


# Import statements
"""

#import statements
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import zscore
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score,roc_curve, auc
from sklearn.preprocessing import StandardScaler, OneHotEncoder,PolynomialFeatures
from sklearn.impute import SimpleImputer

# Import classifiers
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,StackingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBClassifier

"""Loading the dataset"""

# Load dataset
data = pd.read_csv(r"heart_failure_clinical_records_dataset.csv")

"""## #Preprocessing the Data"""

data.head(10)

data.tail(10)

data.describe()

data.info()

data.dtypes
# displays datatypes of each feature

# Split dataset into features and target variable
X = data.drop(columns=['DEATH_EVENT'])
y = data['DEATH_EVENT']

"""## Balancing the dataset

Using SMOTE method
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns
# Apply SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Verify that the number of samples in X_resampled and y_resampled are the same
assert X_resampled.shape[0] == y_resampled.shape[0], "The number of samples in features and target after resampling do not match!"

# Check the new class distribution
print("Original Class Distribution:")
print(y.value_counts())
print("\nResampled Class Distribution:")
print(y_resampled.value_counts())

# Visualize the resampled class distribution
plt.figure(figsize=(10, 6))
sns.countplot(x=y_resampled, palette='viridis')
plt.title('Resampled Class Distribution')
plt.xlabel('Class')
plt.ylabel('Frequency')
plt.show()

"""Checking and Handling missing values"""

#check for the missing values
print(data.isnull().sum())

"""Encoding Categorial Values"""

# Encode categorical variables
encoder = OneHotEncoder(drop='first')
categorical_cols = ['sex']
data_encoded = pd.DataFrame(encoder.fit_transform(data[categorical_cols]).toarray(), columns=encoder.get_feature_names_out(categorical_cols))
data_processed = pd.concat([data, data_encoded], axis=1)
data_processed = data_processed.drop(columns=categorical_cols)

"""### #Detecting and Handling Outliers"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest

# Load a sample dataset
data = pd.read_csv(r"heart_failure_clinical_records_dataset.csv")

# Z-Score Method with capping
def cap_outliers_zscore(data, threshold=3):
    mean = np.mean(data)
    std = np.std(data)
    upper_limit = mean + threshold * std
    lower_limit = mean - threshold * std
    data_capped = np.clip(data, lower_limit, upper_limit)
    return data_capped

# IQR Method
# def detect_outliers_iqr(data):
#     q1 = np.percentile(data, 25)
#     q3 = np.percentile(data, 75)
#     iqr = q3 - q1
#     lower_bound = q1 - 1.5 * iqr
#     upper_bound = q3 + 1.5 * iqr

#     outliers = data[(data < lower_bound) | (data > upper_bound)]
#     return outliers

# Select a numeric column from the dataset
columns = ["age", "anaemia", "creatinine_phosphokinase", "diabetes", "ejection_fraction", "high_blood_pressure", "platelets", "serum_creatinine", "serum_sodium", "sex", "smoking", "time", "DEATH_EVENT"]

original_shape = data.shape
data_capped = data.copy()  # Initialize this with the original data

for column in columns:
    data_column = data[column]

    # Detect and cap outliers using Z-Score method
    data_column_capped = cap_outliers_zscore(data_column)
    print(f"Data after Z-Score capping for {column}: {data_column_capped}")


    # outliers_iqr = detect_outliers_iqr(data_column)
    # print(f"Outliers detected using IQR method in {column}: {outliers_iqr.values}")

    # # Isolation Forest
    # iso = IsolationForest(contamination=0.1, random_state=42)
    # outliers_iso = iso.fit_predict(data_column.values.reshape(-1, 1))
    # outliers_iso = data_column[outliers_iso == -1]
    # print(f"Outliers detected using Isolation Forest in {column}: {outliers_iso.values}")

    # Update the column in the capped dataset
    data_capped[column] = data_column_capped

    # Visualizing the original data using boxplot
    plt.figure(figsize=(10, 5))
    sns.boxplot(x=data_column)
    plt.title(f"Boxplot of Original {column}")
    plt.show()

    # Visualizing the capped data using boxplot
    plt.figure(figsize=(10, 5))
    sns.boxplot(x=data_column_capped)
    plt.title(f"Boxplot of Z-Score Capped {column}")
    plt.show()

# Print the shape of the original dataset and the dataset after capping outliers
print(f"Original dataset shape: {original_shape}")
print(f"Dataset shape after Z-Score capping: {data_capped.shape}")

"""### Feature Engineering"""

# Feature engineering
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_poly = poly.fit_transform(X_resampled)

# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_poly)

"""## Splitting and training the dataset"""

# Check the shape of your arrays before splitting
print("Shape of X_scaled:", X_scaled.shape)
print("Shape of y_resampled:", y_resampled.shape)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled,test_size=0.25, random_state=42)

"""### Define classifiers"""

param_grids={
    "Gradient Boosted1": {
        'n_estimators': [50, 100, 200],
        'learning_rate': [0.01, 0.05, 0.1, 0.2],
        'max_depth': [3, 4, 5, 6],
        'subsample': [0.7, 0.8, 0.9, 1.0],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    },
}
classifiers = {
    "Naive Bayes": GaussianNB(),
    "Decision Tree": GridSearchCV(DecisionTreeClassifier(random_state=1000), param_grid={'max_depth': [3, 5, 7], 'min_samples_leaf': [1, 3, 5]}, cv=5),
    "Random Forest": GridSearchCV(RandomForestClassifier(random_state=42), param_grid={'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}, cv=5),
    "SVM": GridSearchCV(SVC(probability=True), param_grid={'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}, cv=5),
    "Logistic Regression": GridSearchCV(LogisticRegression(), param_grid={'C': [0.1, 1, 10]}, cv=5),
     "Gradient Boosted": RandomizedSearchCV(GradientBoostingClassifier(random_state=42), param_grids["Gradient Boosted1"], n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1),

}
# Store results
results = {
    'Classifier': [],
    'Accuracy': [],
    'Precision': [],
    'Sensitivity': [],
    'Specificity': [],
    'F1 Score': []
}

"""### Calculating the perfomance metrics"""

# Iterate over classifiers
for name, classifier in classifiers.items():
    print(f"Classifier: {name}")

    # Train the classifier
    classifier.fit(X_train, y_train)

    # Obtain predictions from the model
    y_pred = classifier.predict(X_test)

    # Generate the confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    print("Confusion Matrix:")
    print(cm)

    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)
    print("Accuracy:", accuracy)

    # Calculate precision
    precision = precision_score(y_test, y_pred)
    print("Precision:", precision)

    # Calculate recall (sensitivity)
    recall = recall_score(y_test, y_pred)
    print("Sensitivity:", recall)

    # Calculate F1-score
    f1 = f1_score(y_test, y_pred)
    print("F1 Score:", f1)

    # Calculate specificity
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp)
    print("Specificity:", specificity)

        # Store results
    results['Classifier'].append(name)
    results['Accuracy'].append(accuracy)
    results['Precision'].append(precision)
    results['Sensitivity'].append(recall)
    results['Specificity'].append(specificity)
    results['F1 Score'].append(f1)

    print("\n")

"""### ROC curve and AUC"""

# Iterate over classifiers to fit and calculate ROC curves
plt.figure(figsize=(10, 8))

for name, classifier in classifiers.items():
    print(f"Classifier: {name}")
    # Train the classifier with the best parameters
    classifier.fit(X_train, y_train)
    best_classifier = classifier.best_estimator_ if hasattr(classifier, 'best_estimator_') else classifier

    # Obtain predicted probabilities
    y_probs = best_classifier.predict_proba(X_test)[:, 1]

    # Compute ROC curve and AUC
    fpr, tpr, _ = roc_curve(y_test, y_probs)
    roc_auc = auc(fpr, tpr)

    # Plot ROC curve
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

# Plot diagonal line for random classifier
plt.plot([0, 1], [0, 1], 'k--')

# Customize plot
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison of Different Classifiers')
plt.legend(loc='lower right')
plt.show()

"""### Bar plots of perfomance metrics"""

# Convert results to DataFrame
results_df = pd.DataFrame(results)

# Define colors for each classifier
colors = sns.color_palette("husl", len(classifiers))

# Plot the performance metrics
metrics = ['Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1 Score']

for metric in metrics:
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Classifier', y=metric, data=results_df, palette=colors)
    plt.title(f'Comparison of {metric} Across Classifiers')
    plt.xticks(rotation=45)
    plt.ylabel(metric)
    plt.show()

"""# Data Visualization

### Confusion Matrix Heatmap
"""

# Plot confusion matrix heatmap
for name, classifier in classifiers.items():
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=['Predicted Positive','Predicted Negative' ],
             yticklabels=['Actual Positive', 'Actual Negative'])
        plt.title(f"Confusion Matrix - {name}")
        plt.ylabel("Predicted Label")
        plt.xlabel("True Label")
        plt.show()

#missing value Heatmap
sns.heatmap(data.corr(),cmap='coolwarm')
plt.title("Missing data analysis")
plt.show()

sns.scatterplot( data=data )
plt.title('Scatter Plot')
plt.show()

sns.scatterplot( x='serum_sodium',y='creatinine_phosphokinase', data=data )
plt.title('Scatter Plot')
plt.show()

sns.scatterplot( x='serum_sodium',y='platelets', data=data )
plt.title('Scatter Plot')
plt.show()

# Pair Plot
sns.pairplot(data=data)
plt.title('Pair Plot')
plt.show()

#Histogram
#This can help understand the range and spread of each feature
data.hist(alpha=0.5, figsize=(20,10))
plt.suptitle("Histogram")
plt.show()

# Violin Plot
#combines Boxplot with KDE plot
plt.figure(figsize=(10, 6))
sns.violinplot( x="smoking", y='serum_sodium',data=data)
plt.title('Violin Plot')

plt. show()

# Box Plot
sns.boxplot( data=data, x="platelets",y="smoking")
plt.title('Box Plot ')
plt.figure(figsize=(10, 20))
plt.show()

# Contour Plot
sns.kdeplot( x='age', y='platelets', data=data, fill=True)
plt.title('Contour Plot of age and platelets')
plt.show()

plt.figure(figsize=(10, 10))
# Compute correlations and plot heatmap
sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap (Numeric Data Only)')
plt.show()

# Joint Plot
sns. jointplot( x='serum_sodium',y='platelets', data=data, kind='reg')
plt.title('Joint Plot ')
plt.show()

"""# Complete Code


"""

#import statements
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import zscore
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score,roc_curve, auc
from sklearn.preprocessing import StandardScaler, OneHotEncoder,PolynomialFeatures
from sklearn.impute import SimpleImputer
# Import classifiers
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,StackingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

# Load dataset
data = pd.read_csv(r"heart_failure_clinical_records_dataset.csv")

# Split dataset into features and target variable
X = data.drop(columns=['DEATH_EVENT'])
y = data['DEATH_EVENT']



#SMOTE METHOD
import pandas as pd
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns
# Apply SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Verify that the number of samples in X_resampled and y_resampled are the same
assert X_resampled.shape[0] == y_resampled.shape[0], "The number of samples in features and target after resampling do not match!"

# Check the new class distribution
print("Original Class Distribution:")
print(y.value_counts())
print("\nResampled Class Distribution:")
print(y_resampled.value_counts())

# Visualize the resampled class distribution
plt.figure(figsize=(10, 6))
sns.countplot(x=y_resampled, palette='viridis')
plt.title('Resampled Class Distribution')
plt.xlabel('Class')
plt.ylabel('Frequency')
plt.show()



# Encode categorical variables
encoder = OneHotEncoder(drop='first')
categorical_cols = ['sex']
data_encoded = pd.DataFrame(encoder.fit_transform(data[categorical_cols]).toarray(), columns=encoder.get_feature_names_out(categorical_cols))
data_processed = pd.concat([data, data_encoded], axis=1)
data_processed = data_processed.drop(columns=categorical_cols)


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest

# Load a sample dataset
data = pd.read_csv(r"heart_failure_clinical_records_dataset.csv")

# Z-Score Method with capping
def cap_outliers_zscore(data, threshold=3):
    mean = np.mean(data)
    std = np.std(data)
    upper_limit = mean + threshold * std
    lower_limit = mean - threshold * std
    data_capped = np.clip(data, lower_limit, upper_limit)
    return data_capped

# Select a numeric column from the dataset
columns = ["age", "anaemia", "creatinine_phosphokinase", "diabetes", "ejection_fraction", "high_blood_pressure", "platelets", "serum_creatinine", "serum_sodium", "sex", "smoking", "time", "DEATH_EVENT"]

original_shape = data.shape
data_capped = data.copy()  # Initialize this with the original data

for column in columns:
    data_column = data[column]

    # Detect and cap outliers using Z-Score method
    data_column_capped = cap_outliers_zscore(data_column)
    print(f"Data after Z-Score capping for {column}: {data_column_capped}")


    # Update the column in the capped dataset
    data_capped[column] = data_column_capped

    # Visualizing the original data using boxplot
    plt.figure(figsize=(10, 5))
    sns.boxplot(x=data_column)
    plt.title(f"Boxplot of Original {column}")
    plt.show()

    # Visualizing the capped data using boxplot
    plt.figure(figsize=(10, 5))
    sns.boxplot(x=data_column_capped)
    plt.title(f"Boxplot of Z-Score Capped {column}")
    plt.show()

# Print the shape of the original dataset and the dataset after capping outliers
print(f"Original dataset shape: {original_shape}")
print(f"Dataset shape after Z-Score capping: {data_capped.shape}")


###################

# Feature engineering
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_poly = poly.fit_transform(X_resampled)

# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_poly)

# Check the shape of your arrays before splitting
print("Shape of X_scaled:", X_scaled.shape)
print("Shape of y_resampled:", y_resampled.shape)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled,test_size=0.25, random_state=42)


# ########################
param_grids={
    "Gradient Boosted1": {
        'n_estimators': [50, 100, 200, 500],
        'learning_rate': [0.01, 0.05, 0.1, 0.2],
        'max_depth': [3, 4, 5, 6],
        'subsample': [0.7, 0.8, 0.9, 1.0],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    },
}
classifiers = {
    "Naive Bayes": GaussianNB(),
    "Decision Tree": GridSearchCV(DecisionTreeClassifier(random_state=1000), param_grid={'max_depth': [3, 5, 7], 'min_samples_leaf': [1, 3, 5]}, cv=5),
    "Random Forest": GridSearchCV(RandomForestClassifier(random_state=42), param_grid={'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}, cv=5),
    "SVM": GridSearchCV(SVC(probability=True), param_grid={'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}, cv=5),
    "Logistic Regression": GridSearchCV(LogisticRegression(), param_grid={'C': [0.1, 1, 10]}, cv=5),
     "Gradient Boosted": RandomizedSearchCV(GradientBoostingClassifier(random_state=42), param_grids["Gradient Boosted1"], n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1),

}
# Store results
results = {
    'Classifier': [],
    'Accuracy': [],
    'Precision': [],
    'Sensitivity': [],
    'Specificity': [],
    'F1 Score': []
}

# ####

# # Iterate over classifiers
for name, classifier in classifiers.items():
    print(f"Classifier: {name}")

    # Train the classifier
    classifier.fit(X_train, y_train)

    # Obtain predictions from the model
    y_pred = classifier.predict(X_test)

    # Generate the confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    print("Confusion Matrix:")
    print(cm)

    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)
    print("Accuracy:", accuracy)

    # Calculate precision
    precision = precision_score(y_test, y_pred)
    print("Precision:", precision)

    # Calculate recall (sensitivity)
    recall = recall_score(y_test, y_pred)
    print("Sensitivity:", recall)

    # Calculate F1-score
    f1 = f1_score(y_test, y_pred)
    print("F1 Score:", f1)

    # Calculate specificity
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp)
    print("Specificity:", specificity)

        # Store results
    results['Classifier'].append(name)
    results['Accuracy'].append(accuracy)
    results['Precision'].append(precision)
    results['Sensitivity'].append(recall)
    results['Specificity'].append(specificity)
    results['F1 Score'].append(f1)

    print("\n")

# #####

# # ROC AUC Curve

# Iterate over classifiers to fit and calculate ROC curves
plt.figure(figsize=(10, 8))

for name, classifier in classifiers.items():
    print(f"Classifier: {name}")
    # Train the classifier with the best parameters
    classifier.fit(X_train, y_train)
    best_classifier = classifier.best_estimator_ if hasattr(classifier, 'best_estimator_') else classifier

    # Obtain predicted probabilities
    y_probs = best_classifier.predict_proba(X_test)[:, 1]

    # Compute ROC curve and AUC
    fpr, tpr, _ = roc_curve(y_test, y_probs)
    roc_auc = auc(fpr, tpr)

    # Plot ROC curve
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

# Plot diagonal line for random classifier
plt.plot([0, 1], [0, 1], 'k--')

# # Customize plot
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison of Different Classifiers')
plt.legend(loc='lower right')
plt.show()


# #########

# # Convert results to DataFrame
results_df = pd.DataFrame(results)

# # Define colors for each classifier
colors = sns.color_palette("husl", len(classifiers))

# # Plot the performance metrics
metrics = ['Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1 Score']

for metric in metrics:
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Classifier', y=metric, data=results_df, palette=colors)
    plt.title(f'Comparison of {metric} Across Classifiers')
    plt.xticks(rotation=45)
    plt.ylabel(metric)
    plt.show()

###########


###################

knn= GridSearchCV(KNeighborsClassifier(), param_grid={'n_neighbors': [3, 5, 7]}, cv=5)
  # Train the classifier
knn.fit(X_train, y_train)

  # Obtain predictions from the model
y_pred = classifier.predict(X_test)
print("Knn Classifier")
    # Generate the confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

    # Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

    # Calculate precision
precision = precision_score(y_test, y_pred)
print("Precision:", precision)

    # Calculate recall (sensitivity)
recall = recall_score(y_test, y_pred)
print("Sensitivity:", recall)

    # Calculate F1-score
f1 = f1_score(y_test, y_pred)
print("F1 Score:", f1)

    # Calculate specificity
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""# XBG BOOST"""

from xgboost import XGBClassifier

# Define XGBoost parameters
xgb_params = {
    'n_estimators': [ 100, 200, 500,700],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 4, 5, 6],
    'subsample': [0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
}

# RandomizedSearchCV for XGBoost
xgb_classifier = RandomizedSearchCV(XGBClassifier(random_state=67), xgb_params, n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)
xgb_classifier.fit(X_train, y_train)
y_pred_xgb = xgb_classifier.predict(X_test)
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
precision_xgb = precision_score(y_test, y_pred_xgb)
recall_xgb = recall_score(y_test, y_pred_xgb)
f1_xgb = f1_score(y_test, y_pred_xgb)
cm_xgb = confusion_matrix(y_test, y_pred_xgb)
tn_xgb, fp_xgb, fn_xgb, tp_xgb = cm_xgb.ravel()
specificity_xgb = tn_xgb / (tn_xgb + fp_xgb)
print(f"XGBoost - Accuracy: {accuracy_xgb}\n, Precision: {precision_xgb},\n Recall: {recall_xgb}, \nF1 Score: {f1_xgb},\n Specificity: {specificity_xgb}")


# ROC curve for all models including stacking
plt.figure(figsize=(10, 8))
for name, classifier in classifiers.items():
    best_classifier = classifier.best_estimator_ if hasattr(classifier, 'best_estimator_') else classifier
    y_prob = best_classifier.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

# Add Stacking Classifier
y_prob_stack = stacking_clf.predict_proba(X_test)[:, 1]
fpr_stack, tpr_stack, _stack = roc_curve(y_test, y_prob_stack)
roc_auc_stack = auc(fpr_stack, tpr_stack)
plt.plot(fpr_stack, tpr_stack, label=f'Stacking (AUC = {roc_auc_stack:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

"""# Stacking"""

# Stacking
estimators = [
    ('rf', RandomForestClassifier(random_state=42)),
    ('gb', GradientBoostingClassifier(random_state=42)),
    ('xbg', XGBClassifier(random_state=67)), # Changed this line to a proper tuple
]
stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())
stacking_clf.fit(X_train, y_train)

# Confusion matrix for Stacking
cm_stack = confusion_matrix(y_test, y_pred_stack)
print("Stacking Confusion Matrix:")
print(cm_stack)

# Accuracy for Stacking
accuracy_stack = accuracy_score(y_test, y_pred_stack)
print("Stacking Accuracy:", accuracy_stack)

# Precision for Stacking
precision_stack = precision_score(y_test, y_pred_stack)
print("Stacking Precision:", precision_stack)

# Recall (sensitivity) for Stacking
recall_stack = recall_score(y_test, y_pred_stack)
print("Stacking Sensitivity:", recall_stack)

# F1-score for Stacking
f1_stack = f1_score(y_test, y_pred_stack)
print("Stacking F1 Score:", f1_stack)

# Specificity for Stacking
tn_stack, fp_stack, fn_stack, tp_stack = cm_stack.ravel()
specificity_stack = tn_stack / (tn_stack + fp_stack)
print("Stacking Specificity:", specificity_stack)

# # ROC curve for Stacking
y_prob_stack = stacking_clf.predict_proba(X_test)[:, 1]
fpr_stack, tpr_stack, _stack = roc_curve(y_test, y_prob_stack)
roc_auc_stack = auc(fpr_stack, tpr_stack)
plt.plot(fpr_stack, tpr_stack, label=f'Stacking (AUC = {roc_auc_stack:.2f})')

# Plot the ROC curve for all classifiers
plt.figure(figsize=(10, 8))
for name, classifier in classifiers.items():
    best_estimator = classifier.best_estimator_ if hasattr(classifier, 'best_estimator_') else classifier
    y_prob = best_estimator.predict_proba(X_test)[:, 1] if hasattr(best_estimator, 'predict_proba') else best_estimator.decision_function(X_test)
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

plt.plot(fpr_stack, tpr_stack, label=f'Stacking (AUC = {roc_auc_stack:.2f})')
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()